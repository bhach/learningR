Missing Data:
=============
1. MCAR: Data that is missing completely at random
2. MAR: Data that is missing at random
3. MNAR: Data that is missing not at random


MACR:
-----
Circumstances cause some data to be missing from your data set.
No relationship between the missing data and circumstances.
Very rare in real life.
MACR data can be ignored.

For example, imagine that you have a spreadsheet of results from a data collection effort and the rows in that spreadsheet are sorted according to a randomly generated identifier.
If something happens to the spreadsheet and you lose the last 100 rows, that's a situation where the data is missing completely at random.
The order of the rows was completely dependent upon a random variable, and the only thing that the rows had in common was that their random values were the 100 highest in the data set.

MAR:
----
Some underlying circumstances that explain the way that data is missing.
Those circumstances are explained by other variables in the data set.
MAR can be usually ignored.

MNAR:
-----
In these cases, we're once again missing some information, but the value of the mission variable is related to the reason that that variable is missing.

Imagine, for example, that we're measuring blood pressure of individuals and we have a meter that is only able to measure blood pressure values up to 180.
If there are individuals in our population with blood pressures over 180, those value will be missing because they were too high to read.
Situations where data is missing not at random are very serious because the absence of the missing data will impact your conclusions.

Sources of Missing Data:
------------------------
1. Refusal to provide
2. Inapplicable data element
3. Data not yet collected.



ASK THIS QUESTION: Is this value missing becuase it wasn't recorded or becuase it dosen't exist?

If a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN.

On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called "imputation" and we'll learn how to do it next! :)

If you're in a hurry or don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: I don't generally recommend this approch for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)


Note:
-----
Missing Data occurs while converting the formats/units. For example, sometimes numbers are stored as text. While converting, it may lead to NAs.
Units, Currencies, Spelling Mistakes needs to be checked.
Look at summary statistics always
Look at sample rows of data: sample_n(data,5)
Always check percentage of missing data in column wise, rowwise and in total. Check this before and after handling missing data.

Identifying Missing Data (columnwise):
--------------------------------------
1. sapply(mtcars, function(x) sum(is.na(x)))

2. mtcars %>%
  select(everything()) %>%  # replace to your needs
  summarise_all(funs(sum(is.na(.))))

3. apply(nfl_data,MARGIN=2,FUN=function(x){sum(is.na(x))})



Identifying Missing Data (rowwise):
------------------------------------
mtcars %>%
  rowwise %>%
  summarise(NA_per_row = sum(is.na(.)))
  
  
  
select cases where atleast one NA in a row:
-------------------------------------------
myDT <- DT[!complete.cases(V1,V2), ]
V1,V2 are col names in DT



Identifying Missing Values:
---------------------------
There charcters in the column of numeric values
There can be symbols like ""," ",".", etc....
data coercion will not happen sometimes due to this reason as multiple data types present in single columns.

missing values are generally easy to detect. We can simply look for NA values or unusual flags in a file.

Some values are invalid. Find those and treat them as missing values.
For example, in the column of Age, if th value is -12. Its an invalid value.

sapply(dat, function(x) sum(is.na(x)))


Treatments for Missing Data:
----------------------------
1. Figure: use other sources to recontruct the missing data
2. Interpolate: use inferences in other data points to reconstruct the missing data.
3. Ignore:continue without using the missing data.


Treatment for Missing rows (by Imputation):
-----------------------------------------
Rows may be missing completely at random, missing at random, or missing not at random.
Missing rows are usually much more difficult to detect because there's no row there to analyze.
Detecting missing rows generally requires some additional subject matter knowledge. 

For example, if we know that there is supposed to be a row in a data set for every year and we have rows for 1990 through 2016 and then another row for 2018, it's logical to conclude that 2017 is missing.


Treatment for Missing rows (by deletion):
-----------------------------------------
If you have large number of observations in your dataset, where all the classes to be predicted are sufficiently represented in the training data and the model doesn’t lose power.
Try deleting (or not to include missing values while model building)

newdata <- select(msleep, genus, vore, conservation)
newdata <- na.omit(newdata)

na.omit and drop_na would be useful here.

delete columns with X% of missing data



Treatment for Missings columns (by deletion):
---------------------------------------------
If a paricular variable is having more missing values that rest of the variables in the dataset, and, if by removing that one variable you can save many observations, then you are better off without that variable unless it is a really important predictor that makes a lot of business sense.
It is a matter of deciding between the importance of the variable and losing out on a number of observations.

Here,we delete variables (columns) that contain too many missing values.

#Find the proportion of missing data
pctmiss <- colSums(is.na(msleep))/nrow(msleep)

Sixty-one percent of the sleep_cycle values are missing. You may decide to drop it.

dat <- na.omit(dat)

Treatment using Aggregate functions:
------------------------------------
mean()
median()
mode()
min()
max()
count()

Treatment using Prediction:
---------------------------
1. kNN:
=======
uses k-Nearest Neighbours approach to impute missing values.

What kNN imputation does in simpler terms is as follows:
For every observation to be imputed, it identifies ‘k’ closest observations based on the euclidean distance and computes the weighted average (weighted based on distance) of these ‘k’ obs.

The advantage is that you could impute all the missing values in all variables with one call to the function. It takes the whole data frame as the argument and you don’t even have to specify which variabe you want to impute. 


2. rpart:
=========
The limitation with DMwR::knnImputation is that it sometimes may not be appropriate to use when the missing value comes from a factor variable.
Both rpart and mice has flexibility to handle that scenario. The advantage with rpart is that you just need only one of the variables to be non NA in the predictor fields.

3. mice:
========
mice short for Multivariate Imputation by Chained Equations is an R package that provides advanced features for missing value treatment.


look at the summary statistics always

sample_n(nfl_data,5)   # sample 5 rows


functins to find missing values:
sapply(mtcars, function(x) sum(is.na(x)))
or
mtcars %>%
  select(everything()) %>%  # replace to your needs
  summarise_all(funs(sum(is.na(.))))
or
apply(nfl_data,MARGIN=2,FUN=function(x){sum(is.na(x))})


Finding NAs rowwise:
mtcars %>%
  rowwise %>%
  summarise(NA_per_row = sum(is.na(.)))


nfl_data<-read_csv("../input/nflplaybyplay2009to2016/NFL Play by Play 2009-2017 (v4).csv")
permits_data <- read_csv("../input/building-permit-applications-data/Building_Permits.csv")



nfl_missing <- sapply(nfl_data, function(x) {sum(is.na(x))})
nfl_missing


nfl_missing<-as.data.frame(nfl_missing)
col_names <- rownames(nfl_missing)
nfl_missing[,2] <- nfl_missing*100 / nrow(nfl_data)
nfl_missing

ASK THIS QUESTION: Is this value missing becuase it wasn't recorded or becuase it dosen't exist?

If a value is missing becuase it doens't exist (like the height of the oldest child of someone who doesn't have any children) then it doesn't make sense to try and guess what it might be. These values you probalby do want to keep as NaN.

On the other hand, if a value is missing becuase it wasn't recorded, then you can try to guess what it might have been based on the other values in that column and row. (This is called "imputation" and we'll learn how to do it next! :)

If you're in a hurry or don't have a reason to figure out why your values are missing, one option you have is to just remove any rows or columns that contain missing values. (Note: I don't generally recommend this approch for important projects! It's usually worth it to take the time to go through your data and really look at all the columns with missing values one-by-one to really get to know your dataset.)

# just how much data did we lose?


na.omit
drop_na

delete columns with X% of missing data


select cases where atleast one NA:
myDT <- DT[!complete.cases(V1,V2), ]
V1,V2 are col names in DT


5 Powerful R Packages used for imputing missing values:
=======================================================
MICE
Amelia
missForest
Hmisc
mi


https://www.analyticsvidhya.com/blog/2016/03/tutorial-powerful-packages-imputing-missing-values/


MICE Package:
=============
MICE (Multivariate Imputation via Chained Equations) is one of the commonly used package by R users.
Creating multiple imputations as compared to a single imputation (such as mean) takes care of uncertainty in missing values.

MICE assumes that the missing data are Missing at Random (MAR), which means that the probability that a value is missing depends only on observed value and can be predicted using them.
It imputes data on a variable by variable basis by specifying an imputation model per variable.

Suppose we have X1, X2….Xk variables. If X1 has missing values, then it will be regressed on other variables X2 to Xk.
The missing values in X1 will be then replaced by predictive values obtained.

Methods used by this package are:
---------------------------------
PMM (Predictive Mean Matching)  – For numeric variables
logreg(Logistic Regression) – For Binary Variables( with 2 levels)
polyreg(Bayesian polytomous regression) – For Factor Variables (>= 2 levels)
Proportional odds model (ordered, >= 2 levels)


Example1: IRIS
--------------
install.packages("missForest")
library(missForest)
#Generate 10% missing values at Random
iris.mis <- prodNA(iris, noNA = 0.1)

#remove categorical variables
iris.mis <- subset(iris.mis, select = -c(Species))

install.packages("VIM")
library(VIM)
mice_plot <- aggr(iris.mis, col=c('navyblue','yellow'),
                    numbers=TRUE, sortVars=TRUE,
                    labels=names(iris.mis), cex.axis=.7,
                    gap=3, ylab=c("Missing data","Pattern"))

imputed_Data <- mice(iris.mis, m=5, maxit = 50, method = 'pmm', seed = 500)

#check imputed values
imputed_Data$imp$Sepal.Width

Since there are 5 imputed data sets, you can select any using complete() function.

#get complete data ( 2nd out of 5)
completeData <- complete(imputed_Data,2)

if you wish to build models on all 5 datasets, you can do it in one go using with() command. You can also combine the result from these models and obtain a consolidated output using pool() command.

#build predictive model
fit <- with(data = iris.mis, exp = lm(Sepal.Width ~ Sepal.Length + Petal.Width)) 

#combine results of all 5 models
combine <- pool(fit)
summary(combine)


Example2: SLEEP
---------------
#Filling missing values in sleep
md.pattern(sleep)

# Imputing the values using mice
imputed_sleep <- mice(sleep, m=5, method = 'pmm', seed = 101)

# checking the summary
summary(imputed_sleep)

# Checking imputed values of sleep variable
imputed_sleep$imp$Sleep

# Get complete data with 3rd imputation
wholeData <- complete(imputed_sleep,3)

The predictorMatrix is an argument to the mice function.
It specifies the target variable or block in the rows, and the predictor variables on the columns.
An entry of 0 means that the column variable is NOT used to impute the row variable or block.
A nonzero value indicates that it is used.

# Building regression model
model_fit <- with(data = imputed_sleep, exp = lm(BodyWgt ~ BrainWgt + Sleep)) 

# combining results of all 5 models using pool() function
pooled_output <- pool(model_fit)
summary(pooled_output)

#list of methods in mice
methods(mice)

a<-nhanes
a$hyp <- as.factor(a$hyp)

p<-mice(a,m=5,method=c("","pmm","logreg","pmm"),maxit=20)

# list of 5 impuations values for bmi column
p$imp$bmi


Amelia Package:
===============
ultiple imputation helps to reduce bias and increase efficiency. 
It is enabled with bootstrap based EMB algorithm which makes it faster and robust to impute many variables including cross sectional, time series data etc. 

Assumptions:
------------
1. All variables in a data set have Multivariate Normal Distribution (MVN). It uses means and covariances to summarize data.
2. Missing data is random in nature (Missing at Random).

Methodology:
------------
It takes m bootstrap samples and applies EMB algorithm to each sample.
The m estimates of mean and variances will be different.
Finally, the first set of estimates are used to impute first set of missing values using regression, then second set of estimates are used for second set and so on.

MICE vs Amelia:
---------------
MICE imputes data on variable by variable basis whereas MVN uses a joint modeling approach based on multivariate normal distribution.
MICE is capable of handling different types of variables whereas the variables in MVN need to be normally distributed or transformed to approximate normality.
MICE can manage imputation of variables defined on a subset of data whereas MVN cannot.

This package works best when data has multivariable normal distribution. If not, transformation is to be done to bring data close to normality.

The only thing that you need to be careful about is classifying variables. It has 3 parameters:
idvars – keep all ID variables and other variables which you don’t want to impute
noms – keep nominal variables here

#specify columns and run amelia
amelia_fit <- amelia(iris.mis, m=5, parallel = "multicore", noms = "Species")

#access imputed outputs
> amelia_fit$imputations[[1]]
> amelia_fit$imputations[[2]]
> amelia_fit$imputations[[3]]
> amelia_fit$imputations[[4]]
> amelia_fit$imputations[[5]]

To check a particular column in a data set, use the following commands

> amelia_fit$imputations[[5]]$Sepal.Length

#export the outputs to csv files
> write.amelia(amelia_fit, file.stem = "imputed_data_set")


missForest Package:
===================
missForest is an implementation of random forest algorithm.
It’s a non parametric imputation method applicable to various variable types.

Non-parametric method does not make explicit assumptions about functional form of f (any arbitary function).
Instead, it tries to estimate f such that it can be as close to the data points without seeming impractical.

Methodology:
------------
In simple words, it builds a random forest model for each variable.
Then it uses the model to predict missing values in the variable with the help of observed values.

It yield OOB (out of bag) imputation error estimate.
Moreover, it provides high level of control on imputation process.
It has options to return OOB separately (for each variable) instead of aggregating over the whole data matrix.
This helps to look more closely as to how accurately the model has imputed values for each variable.


#impute missing values, using all parameters as default values
> iris.imp <- missForest(iris.mis)

#check imputed values
> iris.imp$ximp

#check imputation error
> iris.imp$OOBerror

NRMSE is normalized mean squared error. It is used to represent error derived from imputing continuous values.

PFC (proportion of falsely classified) is used to represent error derived from imputing categorical values.

#comparing actual data accuracy
> iris.err <- mixError(iris.imp$ximp, iris.mis, iris)
>iris.err

This suggests that categorical variables are imputed with 6% error and continuous variables are imputed with 15% error.
This can be improved by tuning the values of mtry and ntree parameter.
mtry refers to the number of variables being randomly sampled at each split. ntree refers to number of trees to grow in the forest.








nhanes
sleep



library(missForest)
#Generate 10% missing values at Random
iris.mis <- prodNA(iris, noNA = 0.1)




Important functions:
--------------------
with
pool
within



















insert random NAs into a datset:
--------------------------------
install.packages("missForest")
#Generate 10% missing values at Random
> iris.mis <- prodNA(iris, noNA = 0.1)


do these checks also to find out any special values
is.NaN()
is.infinite()

colSums(is.na(df))  - simle way to see number of NAs in each column










