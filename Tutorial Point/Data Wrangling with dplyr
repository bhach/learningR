Data Wrangling:
==============

What is Data Wrangling?
It is the process of converting and mapping the raw data to the formats needed for further analysis.
It is often referred as "Data Munging" or "Data Transformation".


dplyr Functions:
----------------           
select()	                      
filter()	                    	
group_by()	                   	
summarise()                       
arrange()                        
join()	                         	
mutate()	                   	
Binding
Intersect
union
setdiff
setequal


I am using the sampledata.csv file which contains income generated by states from year 2002 to 2015. 

mydata = read.csv("C:/Users/Sreenu/Desktop/MLDataSets/sampledata.csv")

Selecting Random N Rows:
------------------------
sample_n() function selects random rows from a data frame (or table). 
The second parameter of the function tells R the number of rows to select.
sample_n(mydata,3)


Selecting Random Fraction of Rows:
----------------------------------
sample_frac() function returns randomly N% of rows. 
sample_frac(mydata,0.1)		# it returns randomly 10% of rows


Remove Duplicate Rows based on all the columns (Complete Row):
-------------------------------------------------------------
distinct() function is used to eliminate duplicates.
dim(x1)
x1 = distinct(mydata)

Remove Duplicate Rows based on a Particular column:
---------------------------------------------------
x2 = distinct(mydata, Index, .keep_all= TRUE)
dim(x2)

Remove Duplicates Rows based on multiple columns (AND):
-------------------------------------------------------
we are using two columns - Index, Y2010 to determine uniqueness.
x2 = distinct(mydata, Index, Y2010, .keep_all= TRUE)
dim(x2)

Selecting Columns:
------------------
Selects column "Index", columns from "Y2006" to "Y2008".
mydata2 = select(mydata, State, Y2006:Y2008)

Dropping columns:
-----------------
The minus sign before a column tells R to drop the variable.
mydata2 = select(mydata, -Index, -State)

The above code can also be written like :
mydata2 = select(mydata, -c(Index,State))

Selecting or Dropping columns starts with 'Y':
----------------------------------------------
starts_with() function is used to select columns starts with an alphabet.
mydata3 = select(mydata, starts_with("Y"))
head(mydata3)

Adding a negative sign before starts_with() implies dropping the columns starts with 'Y'
mydata33 = select(mydata, -starts_with("Y"))
head(mydata33)


The following functions helps you to select columns based on their names:
Helpers		Description
=======		===========
starts_with()	Starts with a prefix
ends_with()	Ends with a prefix
contains()	Contains a literal string
matches()	Matches a regular expression
num_range()	Numerical range like x01, x02, x03.
one_of()	Columns in character vector.
everything()	All columns.


Selecting columns contain 'I' in their names:
--------------------------------------------
mydata4 = select(mydata, contains("I"))

Reorder columns:
----------------
The column 'State' in the front and the remaining columns follow that.
mydata5 = select(mydata, State, everything())

Rename column names:
--------------------
It is used to change column name.

we are renaming 'Index' column to 'Index1'.
mydata6 = rename(mydata, Index1=Index)

Removinf rows with NAs:
-----------------------
# sample data
df <- data.frame(a = c('1', NA, '3', NA), b = c('a', 'b', 'c', NA), c = c('e', 'f', 'g', NA))

library(dplyr)

# remove rows where all values are NA:
df %>% filter_all(any_vars(!is.na(.)))
df %>% filter_all(any_vars(complete.cases(.)))  


# remove rows where only some values are NA:
df %>% filter_all(all_vars(!is.na(.)))
df %>% filter_all(all_vars(complete.cases(.)))  

# or more succinctly:
df %>% filter(complete.cases(.))  
df %>% na.omit

# dplyr and tidyr:
library(tidyr)
df %>% drop_na

filter():
---------
It is used to subset data with matching logical conditions.

->Single Selection Criteria:
mydata7 = filter(mydata, Index == "A")

->Multiple Selection Criteria:
The %in% operator can be used to select multiple items.

Select rows against 'A' and 'C' in column 'Index'.
mydata7 = filter(mydata, Index %in% c("A", "C"))

->'AND' Condition in Selection Criteria:
Filtering data for 'A' and 'C' in the column 'Index' and income greater than 13 lakh in Year 2002.
mydata8 = filter(mydata, Index %in% c("A", "C") & Y2002 >= 1300000)

->'OR' Condition in Selection Criteria:
| (OR) in the logical condition. It means any of the two conditions.
mydata9 = filter(mydata, Index %in% c("A", "C") | Y2002 >= 1300000)

->NOT Condition:
The "!" sign is used to reverse the logical condition.
mydata10 = filter(mydata, !Index %in% c("A", "C"))

->CONTAINS Condition:
The grepl() function is used to search for pattern matching. 
we are looking for records wherein column state contains 'Ar' in their name.
mydata10 = filter(mydata, grepl("Ar", State))


summarise():
------------
It is used to summarize data.

Summarize selected columns:
---------------------------
we are calculating mean and median for the column Y2015.
summarise(mydata, Y2015_mean = mean(Y2015), Y2015_med=median(Y2015))


Summarize Multiple Columns:
---------------------------
we are calculating number of records, mean and median for columns Y2005 and Y2006. 
summarise_at() function allows us to select multiple columns by their names.
summarise_at(mydata, vars(Y2005, Y2006), funs(n(), mean, median))


Working on another dataset:
---------------------------
# I am using the airquality dataset from the datasets package. 
# The airquality dataset contains information about air quality measurements in New York from May 1973 – September 1973.

dim(airquality)
names(airquality)
head(airquality)

sample_n(airquality, size = 10)
sample_frac(airquality, size = 0.1)

# we can return all rows with Temp greater than 70 as follows:
filter(airquality, Temp > 70)

# return all rows with Temp larger than 80 and Month higher than 5.
filter(airquality, Temp > 80 & Month > 5)

# adds a new column that displays the temperature in Celsius.
mutate(airquality, TempInC = (Temp - 32) * 5 / 9)

summarise(airquality, mean(Temp, na.rm = TRUE))
summarise(airquality, Temp_mean = mean(Temp, na.rm = TRUE))

# Group By
--> The group_by function is used to group data by one or more columns. 
--> we can group the data together based on the Month, and then use the summarise function to calculate and display the mean temperature for each month.
summarise(group_by(airquality, Month), mean(Temp, na.rm = TRUE))

# Count
--> The count function calculates the no. of observations based on a group. 
--> It is slightly similar to the table function in the base package.
count(airquality, Month)
--> This means that there are 31 rows with Month = 5, 30 rows with Month = 6, and so on.

# Arrange
--> The arrange function is used to arrange rows by columns. 
--> Currently, the airquality dataset is arranged based on Month, and then Day. 
--> We can use the arrange function to arrange the rows in the descending order of Month, and then in the ascending order of Day.
arrange(airquality, desc(Month), Day)

# Pipe
--> The pipe operator in R, represented by %>% can be used to chain code together. 
--> It is very useful when you are performing several operations on data, and don’t want to save the output at each intermediate step.

--> For example, let’s say we want to remove all the data corresponding to Month = 5, group the data by month, and then find the mean of the temperature each month. 
--> The conventional way to write the code for this would be:

filteredData <- filter(airquality, Month != 5)
groupedData <- group_by(filteredData, Month)
summarise(groupedData, mean(Temp, na.rm = TRUE))

--> With piping, the above code can be rewritten as:

airquality %>% 
    filter(Month != 5) %>% 
    group_by(Month) %>% 
    summarise(mean(Temp, na.rm = TRUE))
        

JOINS:
======
The join functions in the dplyr package combine two tables such that matching rows are together.
left_join() only keeps rows that have information in the first table.
right_join() only keeps rows that have information in the second table.
inner_join() only keeps rows that have information in both tables.
full_join() keeps all rows from both tables.
semi_join() keeps the part of first table for which we have information in the second.
anti_join() keeps the elements of the first table for which there is no information in the second.


library(ggrepel)
library(dslabs)

head(murders)
head(results_us_election_2016)


check if states in both tables are same or not:
-----------------------------------------------
identical(results_us_election_2016$state, murders$state)


join the murders table and US election results table:
-----------------------------------------------------
tab <- left_join(murders, results_us_election_2016, by = "state")


tab1 <- slice(murders, 1:6) %>% select(state, population)
tab2 <- slice(results_us_election_2016, c(1:3, 5, 7:8)) %>% select(state, electoral_votes)


experiment with different joins:
--------------------------------
left_join(tab1, tab2)
tab1 %>% left_join(tab2)
tab1 %>% right_join(tab2)
inner_join(tab1, tab2)
semi_join(tab1, tab2)
anti_join(tab1, tab2)

Binding:
========

Unlike the join functions, the binding functions do not try to match by a variable, but rather just combine datasets.

bind_cols():
------------
It binds two objects by making them columns in a tibble. The R-base function cbind() binds columns but makes a data frame or matrix instead.

bind_rows():
------------
It is similar but binds rows instead of columns. The R-base function rbind() binds rows but makes a data frame or matrix instead.


bind_cols(a = 1:3, b = 4:6)

tab1 <- tab[, 1:3]
tab2 <- tab[, 4:6]
tab3 <- tab[, 7:9]
new_tab <- bind_cols(tab1, tab2, tab3)
head(new_tab)

tab1 <- tab[1:2,]
tab2 <- tab[3:4,]
bind_rows(tab1, tab2)

SET:
====
By default, the set operators in R-base work on vectors. If tidyverse/dplyr are loaded, they also work on data frames.

intersect(): This returns the elements common to both sets.
union(): This returns the elements that are in either set.
setdiff(): set difference between a first and second argument can be obtained with setdiff(). Note that this function is not symmetric.
set_equal(): It tells us if two sets are the same, regardless of the order of elements.

intersect vectors or data frames:
---------------------------------
intersect(1:10, 6:15)
intersect(c("a","b","c"), c("b","c","d"))
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
intersect(tab1, tab2)


perform a union of vectors or data frames:
------------------------------------------
union(1:10, 6:15)
union(c("a","b","c"), c("b","c","d"))
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
union(tab1, tab2)


set difference of vectors or data frames:
-----------------------------------------
setdiff(1:10, 6:15)
setdiff(6:15, 1:10)
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
setdiff(tab1, tab2)


setequal determines whether sets have the same elements, regardless of order:
-----------------------------------------------------------------------------
setequal(1:5, 1:6)
setequal(1:5, 5:1)
setequal(tab1, tab2)

unite():
========
unite() fucntion is used to combines two or more columns into a single column.

Let us create some dummy data:
date <- as.Date('2016-01-01') + 0:14
hour <- sample(1:24, 15)
min <- sample(1:60, 15)
second <- sample(1:60, 15)
event <- sample(letters, 15)
data <- data.frame(date, hour, min, second, event)
print(data)

# Now, let us combine the date, hour, min, and second columns into a new column called datetime. 
# Usually, datetime in R is of the form Year-Month-Day Hour:Min:Second.

dataNew <- data %>%
  unite(time, hour, min, second, sep = ':')
print(dataNew)

dataNew <- data %>%
  unite(time, hour, min, second, sep = ':') %>%
  unite(datetime, date, time, sep = ' ')
print(dataNew)


separate():
===========
separate() is used to splits one column into two or more columns.


We can get back the original data we created using separate as follows:
data1 <- dataNew %>% 
  separate(datetime, c('date', 'time'), sep = ' ') %>% 
  separate(time, c('hour', 'min', 'second'), sep = ':')
print(data1)

It first splits the datetime column into date and time, and then splits time into hour, min, and second.