Data Wrangling:
==============

What is Data Wrangling?
It is the process of converting and mapping the raw data to the formats needed for further analysis. It is often referred as "Data Munging" or "Data Transformation".


library(dslabs)

Step1: Import the data
Step2: Convert the data to tidy form





dplyr Functions:
==================

dplyr Function			         Equivalent SQL
---------------             ----------------
select()	                       SELECT
filter()	                    		WHERE
group_by()	                   	GROUP BY
summarise()                        NA
arrange()                        ORDER BY
join()	                         	JOIN
mutate()	                   	COLUMN ALIAS


I am using the sampledata.csv file which contains income generated by states from year 2002 to 2015. 

mydata = read.csv("C:/Users/Sreenu/Desktop/MLDataSets/sampledata.csv")


Selecting Random N Rows:
------------------------
sample_n() function selects random rows from a data frame (or table). 
The second parameter of the function tells R the number of rows to select.
sample_n(mydata,3)


Selecting Random Fraction of Rows:
----------------------------------
sample_frac() function returns randomly N% of rows. 
sample_frac(mydata,0.1)		# it returns randomly 10% of rows


Remove Duplicate Rows based on all the columns (Complete Row):
-------------------------------------------------------------
distinct() function is used to eliminate duplicates.
dim(x1)
x1 = distinct(mydata)
dime(x1)

Remove Duplicate Rows based on a Particular column:
---------------------------------------------------
x2 = distinct(mydata, Index, .keep_all= TRUE)
dim(x2)

Remove Duplicates Rows based on multiple columns (AND):
-------------------------------------------------------
we are using two columns - Index, Y2010 to determine uniqueness.
x2 = distinct(mydata, Index, Y2010, .keep_all= TRUE)
dim(x2)

Selecting Columns:
------------------
Selects column "Index", columns from "Y2006" to "Y2008".
mydata2 = select(mydata, State, Y2006:Y2008)

Dropping columns:
-----------------
The minus sign before a column tells R to drop the variable.
mydata2 = select(mydata, -Index, -State)

The above code can also be written like :
mydata2 = select(mydata, -c(Index,State))

Selecting or Dropping columns starts with 'Y':
----------------------------------------------
starts_with() function is used to select columns starts with an alphabet.
mydata3 = select(mydata, starts_with("Y"))
head(mydata3)

Adding a negative sign before starts_with() implies dropping the columns starts with 'Y'
mydata33 = select(mydata, -starts_with("Y"))
head(mydata33)


The following functions helps you to select columns based on their names:
Helpers		Description
=======		===========
starts_with()	Starts with a prefix
ends_with()	Ends with a prefix
contains()	Contains a literal string
matches()	Matches a regular expression
num_range()	Numerical range like x01, x02, x03.
one_of()	Columns in character vector.
everything()	All columns.

Selecting columns contain 'I' in their names:
--------------------------------------------
mydata4 = select(mydata, contains("I"))

Reorder columns:
----------------
The column 'State' in the front and the remaining columns follow that.
mydata5 = select(mydata, State, everything())


Rename column names:
--------------------
It is used to change column name.

we are renaming 'Index' column to 'Index1'.
mydata6 = rename(mydata, Index1=Index)


filter():
---------
It is used to subset data with matching logical conditions.

->Single Selection Criteria:
mydata7 = filter(mydata, Index == "A")

->Multiple Selection Criteria:
The %in% operator can be used to select multiple items.

Select rows against 'A' and 'C' in column 'Index'.
mydata7 = filter(mydata, Index %in% c("A", "C"))

->'AND' Condition in Selection Criteria:
Filtering data for 'A' and 'C' in the column 'Index' and income greater than 13 lakh in Year 2002.
mydata8 = filter(mydata, Index %in% c("A", "C") & Y2002 >= 1300000)

->'OR' Condition in Selection Criteria:
| (OR) in the logical condition. It means any of the two conditions.
mydata9 = filter(mydata, Index %in% c("A", "C") | Y2002 >= 1300000)

->NOT Condition:
The "!" sign is used to reverse the logical condition.
mydata10 = filter(mydata, !Index %in% c("A", "C"))

->CONTAINS Condition:
The grepl() function is used to search for pattern matching. 
we are looking for records wherein column state contains 'Ar' in their name.
mydata10 = filter(mydata, grepl("Ar", State))


summarise():
------------
It is used to summarize data.

Summarize selected columns:
we are calculating mean and median for the column Y2015.
summarise(mydata, Y2015_mean = mean(Y2015), Y2015_med=median(Y2015))

Summarize Multiple Columns
we are calculating number of records, mean and median for columns Y2005 and Y2006. 
summarise_at() function allows us to select multiple columns by their names.
summarise_at(mydata, vars(Y2005, Y2006), funs(n(), mean, median))


Working on another dataset:
---------------------------
# I am using the airquality dataset from the datasets package. 
# The airquality dataset contains information about air quality measurements in New York from May 1973 – September 1973.

dim(airquality)
names(airquality)
head(airquality)

sample_n(airquality, size = 10)
sample_frac(airquality, size = 0.1)

# we can return all rows with Temp greater than 70 as follows:
filter(airquality, Temp > 70)

# return all rows with Temp larger than 80 and Month higher than 5.
filter(airquality, Temp > 80 & Month > 5)

# adds a new column that displays the temperature in Celsius.
mutate(airquality, TempInC = (Temp - 32) * 5 / 9)

summarise(airquality, mean(Temp, na.rm = TRUE))
summarise(airquality, Temp_mean = mean(Temp, na.rm = TRUE))

# Group By
--> The group_by function is used to group data by one or more columns. 
--> we can group the data together based on the Month, and then use the summarise function to calculate and display the mean temperature for each month.
summarise(group_by(airquality, Month), mean(Temp, na.rm = TRUE))

# Count
--> The count function calculates the no. of observations based on a group. 
--> It is slightly similar to the table function in the base package.
count(airquality, Month)
--> This means that there are 31 rows with Month = 5, 30 rows with Month = 6, and so on.

# Arrange
--> The arrange function is used to arrange rows by columns. 
--> Currently, the airquality dataset is arranged based on Month, and then Day. 
--> We can use the arrange function to arrange the rows in the descending order of Month, and then in the ascending order of Day.
arrange(airquality, desc(Month), Day)

# Pipe
--> The pipe operator in R, represented by %>% can be used to chain code together. 
--> It is very useful when you are performing several operations on data, and don’t want to save the output at each intermediate step.

--> For example, let’s say we want to remove all the data corresponding to Month = 5, group the data by month, and then find the mean of the temperature each month. 
--> The conventional way to write the code for this would be:

filteredData <- filter(airquality, Month != 5)
groupedData <- group_by(filteredData, Month)
summarise(groupedData, mean(Temp, na.rm = TRUE))

--> With piping, the above code can be rewritten as:

airquality %>% 
    filter(Month != 5) %>% 
    group_by(Month) %>% 
    summarise(mean(Temp, na.rm = TRUE))
    
    
tidyr:
======
--> tidyr package is an evolution of reshape2 (2010-2014) and reshape (2005-2010) packages.
--> It's designed specifically for data tidying (not general reshaping or aggregating) 
--> tidyr is new package that makes it easy to “tidy” your data. 
--> it’s easy to munge (with dplyr), visualise (with ggplot2 or ggvis) and model (modelling packages).

Install and Load tidyr:
install.packages("tidyr")
library(tidyr)

Translation b/w the terminology used in different places:
tidyr		gather	spread
====		======	======
reshape(2)	melt	cast
spreadsheets	unpivot	pivot
databases	fold	unfold

# I will use the mtcars dataset from the datasets library.
head(mtcars)
dim(mtcars)

# Let us include the names of the cars in a column called car for easier manipulation.
mtcars$car <- rownames(mtcars)


gather():
=========
--> gather() function is used to converts wide data to longer format. 
--> It is analogous to the melt function from reshape2.

syntax:
gather(data, key, value, ..., na.rm = FALSE, convert = FALSE)
where ... is the specification of the columns to gather.

# We can replicate what melt does as follows:
mtcarsNew <- mtcars %>% gather(attribute, value, -car)
dim(mtcarsNew)
head(mtcarsNew)
tail(mtcarsNew)

--> As we can see, it gathers all the columns except car and places their name and value into the attritube and value column respectively.
--> The great thing about tidyr is that you can gather only certain columns and leave the others alone. 
--> If we want to gather all the columns from mpg to gear and leave the carb and car columns as they are, we can do it as follows:

mtcarsNew <- mtcars %>% gather(attribute, value, mpg:gear)
dim(mtcarsNew)
head(mtcarsNew)

spread():
=========
--> spread() fucntion is used to converts long data to wider format. 
--> It is analogous to the cast function from reshape2.

syntax:
spread(data, key, value, fill = NA, convert = FALSE, drop = TRUE)

We can replicate what cast does as follows:

mtcarsSpread <- mtcarsNew %>% spread(attribute, value)
head(mtcarsSpread)


Wide Format:
1. Each row represents several observations.
2. One of the varaibles is stored in the header

Plotting tidy data is simple compared to wider data

library(tidyverse)
library(dslabs)
data(gapminder)

Wider Format:
-------------
import and inspect example of original Gapminder data in wide format
path <- system.file("extdata", package="dslabs")
filename <- file.path(path,  "fertility-two-countries-example.csv")
wide_data <- read_csv(filename)
select(wide_data, country, `1960`:`1967`)


Creating Tidy Data:
-------------------
create and inspect a tidy data frame
tidy_data <- gapminder %>% 
  filter(country %in% c("South Korea", "Germany")) %>%
  select(country, year, fertility)
head(tidy_data)


gather wide data to make new tidy data:
--------------------------------------
new_tidy_data <- wide_data %>%
  gather(year, fertility, `1960`:`2015`)
head(new_tidy_data)

gather all columns except country:
----------------------------------
new_tidy_data <- wide_data %>%
  gather(year, fertility, -country)

IMP: gather treats column names as characters by default:
---------------------------------------------------------
class(tidy_data$year)
class(new_tidy_data$year)

convert gathered column names to numeric:
-----------------------------------------
Be careful while gathering columns into rows.

new_tidy_data <- wide_data %>%
  gather(year, fertility, -country, convert = TRUE)
class(new_tidy_data$year)

ggplot on converted tidy data:
-------------------------------
ggplot works on new tidy data
new_tidy_data %>%
  ggplot(aes(year, fertility, color = country)) +
  geom_point()

spread tidy data to generate wide data:
---------------------------------------
new_wide_data <- new_tidy_data %>% spread(year, fertility)
select(new_wide_data, country, `1960`:`1967`)


seperate values in a column:
----------------------------
import data
path <- system.file("extdata", package = "dslabs")
filename <- file.path(path, "life-expectancy-and-fertility-two-countries-example.csv")
raw_dat <- read_csv(filename)
select(raw_dat, 1:5)

dat <- raw_dat %>% gather(key, value, -country)

separate on underscores:
------------------------
dat %>% separate(key, c("year", "variable_name"), "_")

split on all underscores, pad empty cells with NA:
--------------------------------------------------
dat %>% separate(key, c("year", "first_variable_name", "second_variable_name"), 
                 fill = "right")

split on first underscore but keep life_expectancy merged:
----------------------------------------------------------
dat %>% separate(key, c("year", "variable_name"), sep = "_", extra = "merge")

separate then spread:
---------------------
dat %>% separate(key, c("year", "variable_name"), sep = "_", extra = "merge") %>%
  spread(variable_name, value) 

# separate then unite
dat %>% 
  separate(key, c("year", "first_variable_name", "second_variable_name"), fill = "right") %>%
  unite(variable_name, first_variable_name, second_variable_name, sep="_")

# full code for tidying data
dat %>% 
  separate(key, c("year", "first_variable_name", "second_variable_name"), fill = "right") %>%
  unite(variable_name, first_variable_name, second_variable_name, sep="_") %>%
  spread(variable_name, value) %>%
  rename(fertility = fertility_NA)


JOINS:
======
The join functions in the dplyr package combine two tables such that matching rows are together.
left_join() only keeps rows that have information in the first table.
right_join() only keeps rows that have information in the second table.
inner_join() only keeps rows that have information in both tables.
full_join() keeps all rows from both tables.
semi_join() keeps the part of first table for which we have information in the second.
anti_join() keeps the elements of the first table for which there is no information in the second.


library(ggrepel)
library(dslabs)

head(murders)
head(results_us_election_2016)


check if states in both tables are same or not:
-----------------------------------------------
identical(results_us_election_2016$state, murders$state)


join the murders table and US election results table:
-----------------------------------------------------
tab <- left_join(murders, results_us_election_2016, by = "state")


tab1 <- slice(murders, 1:6) %>% select(state, population)
tab2 <- slice(results_us_election_2016, c(1:3, 5, 7:8)) %>% select(state, electoral_votes)


experiment with different joins:
--------------------------------
left_join(tab1, tab2)
tab1 %>% left_join(tab2)
tab1 %>% right_join(tab2)
inner_join(tab1, tab2)
semi_join(tab1, tab2)
anti_join(tab1, tab2)

Binding:
========

Unlike the join functions, the binding functions do not try to match by a variable, but rather just combine datasets.

bind_cols():
------------
It binds two objects by making them columns in a tibble. The R-base function cbind() binds columns but makes a data frame or matrix instead.

bind_rows():
------------
It is similar but binds rows instead of columns. The R-base function rbind() binds rows but makes a data frame or matrix instead.


bind_cols(a = 1:3, b = 4:6)

tab1 <- tab[, 1:3]
tab2 <- tab[, 4:6]
tab3 <- tab[, 7:9]
new_tab <- bind_cols(tab1, tab2, tab3)
head(new_tab)

tab1 <- tab[1:2,]
tab2 <- tab[3:4,]
bind_rows(tab1, tab2)

SET:
====
By default, the set operators in R-base work on vectors. If tidyverse/dplyr are loaded, they also work on data frames.

intersect(): This returns the elements common to both sets.
union(): This returns the elements that are in either set.
setdiff(): set difference between a first and second argument can be obtained with setdiff(). Note that this function is not symmetric.
set_equal(): It tells us if two sets are the same, regardless of the order of elements.

intersect vectors or data frames:
---------------------------------
intersect(1:10, 6:15)
intersect(c("a","b","c"), c("b","c","d"))
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
intersect(tab1, tab2)


perform a union of vectors or data frames:
------------------------------------------
union(1:10, 6:15)
union(c("a","b","c"), c("b","c","d"))
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
union(tab1, tab2)


set difference of vectors or data frames:
-----------------------------------------
setdiff(1:10, 6:15)
setdiff(6:15, 1:10)
tab1 <- tab[1:5,]
tab2 <- tab[3:7,]
setdiff(tab1, tab2)


setequal determines whether sets have the same elements, regardless of order:
-----------------------------------------------------------------------------
setequal(1:5, 1:6)
setequal(1:5, 5:1)
setequal(tab1, tab2)
